{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 626,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.032,
      "grad_norm": 11.332357406616211,
      "learning_rate": 1.9850427350427352e-05,
      "loss": 23.0804,
      "step": 10
    },
    {
      "epoch": 0.064,
      "grad_norm": 21.522966384887695,
      "learning_rate": 1.9700854700854702e-05,
      "loss": 22.9791,
      "step": 20
    },
    {
      "epoch": 0.096,
      "grad_norm": 378.9815979003906,
      "learning_rate": 1.952991452991453e-05,
      "loss": 23.3869,
      "step": 30
    },
    {
      "epoch": 0.128,
      "grad_norm": 11.281567573547363,
      "learning_rate": 1.9316239316239317e-05,
      "loss": 22.2576,
      "step": 40
    },
    {
      "epoch": 0.16,
      "grad_norm": 13.877756118774414,
      "learning_rate": 1.9102564102564106e-05,
      "loss": 22.0361,
      "step": 50
    },
    {
      "epoch": 0.192,
      "grad_norm": 14.186294555664062,
      "learning_rate": 1.888888888888889e-05,
      "loss": 21.5814,
      "step": 60
    },
    {
      "epoch": 0.224,
      "grad_norm": 8.510489463806152,
      "learning_rate": 1.867521367521368e-05,
      "loss": 21.1042,
      "step": 70
    },
    {
      "epoch": 0.256,
      "grad_norm": 27.135845184326172,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 20.1658,
      "step": 80
    },
    {
      "epoch": 0.288,
      "grad_norm": 139.60379028320312,
      "learning_rate": 1.8247863247863247e-05,
      "loss": 20.3435,
      "step": 90
    },
    {
      "epoch": 0.32,
      "grad_norm": 20.754825592041016,
      "learning_rate": 1.8034188034188037e-05,
      "loss": 20.0646,
      "step": 100
    },
    {
      "epoch": 0.352,
      "grad_norm": 44.524688720703125,
      "learning_rate": 1.7820512820512823e-05,
      "loss": 20.6268,
      "step": 110
    },
    {
      "epoch": 0.384,
      "grad_norm": 22.864295959472656,
      "learning_rate": 1.760683760683761e-05,
      "loss": 19.9948,
      "step": 120
    },
    {
      "epoch": 0.416,
      "grad_norm": 10.437922477722168,
      "learning_rate": 1.7393162393162395e-05,
      "loss": 19.9461,
      "step": 130
    },
    {
      "epoch": 0.448,
      "grad_norm": 30.12860870361328,
      "learning_rate": 1.7200854700854702e-05,
      "loss": 20.0647,
      "step": 140
    },
    {
      "epoch": 0.48,
      "grad_norm": 8.183531761169434,
      "learning_rate": 1.700854700854701e-05,
      "loss": 19.454,
      "step": 150
    },
    {
      "epoch": 0.512,
      "grad_norm": 6306.5380859375,
      "learning_rate": 1.6794871794871796e-05,
      "loss": 20.2206,
      "step": 160
    },
    {
      "epoch": 0.544,
      "grad_norm": 47.335174560546875,
      "learning_rate": 1.6581196581196585e-05,
      "loss": 19.4849,
      "step": 170
    },
    {
      "epoch": 0.576,
      "grad_norm": 18.75531005859375,
      "learning_rate": 1.6367521367521368e-05,
      "loss": 19.0182,
      "step": 180
    },
    {
      "epoch": 0.608,
      "grad_norm": 182.0094757080078,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 19.269,
      "step": 190
    },
    {
      "epoch": 0.64,
      "grad_norm": 39.56962203979492,
      "learning_rate": 1.5940170940170943e-05,
      "loss": 19.5408,
      "step": 200
    },
    {
      "epoch": 0.672,
      "grad_norm": 20.892391204833984,
      "learning_rate": 1.5726495726495726e-05,
      "loss": 19.0786,
      "step": 210
    },
    {
      "epoch": 0.704,
      "grad_norm": 1413.3604736328125,
      "learning_rate": 1.5512820512820516e-05,
      "loss": 19.2827,
      "step": 220
    },
    {
      "epoch": 0.736,
      "grad_norm": 47.097843170166016,
      "learning_rate": 1.5299145299145298e-05,
      "loss": 18.9756,
      "step": 230
    },
    {
      "epoch": 0.768,
      "grad_norm": 2247.596435546875,
      "learning_rate": 1.5085470085470086e-05,
      "loss": 18.8296,
      "step": 240
    },
    {
      "epoch": 0.8,
      "grad_norm": 6227.80322265625,
      "learning_rate": 1.4871794871794874e-05,
      "loss": 18.6947,
      "step": 250
    },
    {
      "epoch": 0.832,
      "grad_norm": 65.47216033935547,
      "learning_rate": 1.4658119658119658e-05,
      "loss": 19.3535,
      "step": 260
    },
    {
      "epoch": 0.864,
      "grad_norm": 345.0196228027344,
      "learning_rate": 1.4444444444444446e-05,
      "loss": 18.8076,
      "step": 270
    },
    {
      "epoch": 0.896,
      "grad_norm": 35.78963851928711,
      "learning_rate": 1.4252136752136753e-05,
      "loss": 18.8259,
      "step": 280
    },
    {
      "epoch": 0.928,
      "grad_norm": 8892.0556640625,
      "learning_rate": 1.403846153846154e-05,
      "loss": 19.4539,
      "step": 290
    },
    {
      "epoch": 0.96,
      "grad_norm": 115.14159393310547,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 19.4258,
      "step": 300
    },
    {
      "epoch": 0.992,
      "grad_norm": 81.02069854736328,
      "learning_rate": 1.3632478632478635e-05,
      "loss": 18.2305,
      "step": 310
    },
    {
      "epoch": 1.0224,
      "grad_norm": 85.99591064453125,
      "learning_rate": 1.3418803418803419e-05,
      "loss": 18.5825,
      "step": 320
    },
    {
      "epoch": 1.0544,
      "grad_norm": 148.6095428466797,
      "learning_rate": 1.3205128205128207e-05,
      "loss": 18.7209,
      "step": 330
    },
    {
      "epoch": 1.0864,
      "grad_norm": 14.869159698486328,
      "learning_rate": 1.2991452991452993e-05,
      "loss": 18.5683,
      "step": 340
    },
    {
      "epoch": 1.1184,
      "grad_norm": 79.01673126220703,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 18.5231,
      "step": 350
    },
    {
      "epoch": 1.1504,
      "grad_norm": 35.41633224487305,
      "learning_rate": 1.2564102564102565e-05,
      "loss": 18.6637,
      "step": 360
    },
    {
      "epoch": 1.1824,
      "grad_norm": 191.33697509765625,
      "learning_rate": 1.2350427350427353e-05,
      "loss": 18.6022,
      "step": 370
    },
    {
      "epoch": 1.2144,
      "grad_norm": 19.674699783325195,
      "learning_rate": 1.2136752136752137e-05,
      "loss": 18.5728,
      "step": 380
    },
    {
      "epoch": 1.2464,
      "grad_norm": 290.9446716308594,
      "learning_rate": 1.1923076923076925e-05,
      "loss": 18.4966,
      "step": 390
    },
    {
      "epoch": 1.2784,
      "grad_norm": 1689.082763671875,
      "learning_rate": 1.170940170940171e-05,
      "loss": 18.3374,
      "step": 400
    },
    {
      "epoch": 1.3104,
      "grad_norm": 674.918701171875,
      "learning_rate": 1.1495726495726495e-05,
      "loss": 18.5958,
      "step": 410
    },
    {
      "epoch": 1.3424,
      "grad_norm": 147.9784698486328,
      "learning_rate": 1.1282051282051283e-05,
      "loss": 18.4035,
      "step": 420
    },
    {
      "epoch": 1.3744,
      "grad_norm": 1443.4876708984375,
      "learning_rate": 1.106837606837607e-05,
      "loss": 17.7779,
      "step": 430
    },
    {
      "epoch": 1.4064,
      "grad_norm": 27.16559410095215,
      "learning_rate": 1.0854700854700855e-05,
      "loss": 18.1069,
      "step": 440
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 13.593412399291992,
      "learning_rate": 1.0641025641025643e-05,
      "loss": 17.7225,
      "step": 450
    },
    {
      "epoch": 1.4704,
      "grad_norm": 2426.24853515625,
      "learning_rate": 1.0427350427350429e-05,
      "loss": 18.3068,
      "step": 460
    },
    {
      "epoch": 1.5024,
      "grad_norm": 111.12918090820312,
      "learning_rate": 1.0213675213675213e-05,
      "loss": 18.3089,
      "step": 470
    },
    {
      "epoch": 1.5344,
      "grad_norm": 4707.46240234375,
      "learning_rate": 1e-05,
      "loss": 18.7928,
      "step": 480
    },
    {
      "epoch": 1.5664,
      "grad_norm": 2021.7742919921875,
      "learning_rate": 9.786324786324787e-06,
      "loss": 18.1659,
      "step": 490
    },
    {
      "epoch": 1.5984,
      "grad_norm": 374.0022888183594,
      "learning_rate": 9.572649572649575e-06,
      "loss": 18.172,
      "step": 500
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 58.51118087768555,
      "learning_rate": 9.358974358974359e-06,
      "loss": 17.9693,
      "step": 510
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 748.7894287109375,
      "learning_rate": 9.145299145299145e-06,
      "loss": 18.3444,
      "step": 520
    },
    {
      "epoch": 1.6944,
      "grad_norm": 25.849628448486328,
      "learning_rate": 8.931623931623933e-06,
      "loss": 18.1638,
      "step": 530
    },
    {
      "epoch": 1.7264,
      "grad_norm": 3194.17919921875,
      "learning_rate": 8.717948717948719e-06,
      "loss": 17.7704,
      "step": 540
    },
    {
      "epoch": 1.7584,
      "grad_norm": 15.488184928894043,
      "learning_rate": 8.504273504273505e-06,
      "loss": 17.8973,
      "step": 550
    },
    {
      "epoch": 1.7904,
      "grad_norm": 46.803680419921875,
      "learning_rate": 8.311965811965812e-06,
      "loss": 19.0559,
      "step": 560
    },
    {
      "epoch": 1.8224,
      "grad_norm": 53.3198127746582,
      "learning_rate": 8.098290598290598e-06,
      "loss": 17.9576,
      "step": 570
    },
    {
      "epoch": 1.8544,
      "grad_norm": 1249.428955078125,
      "learning_rate": 7.884615384615384e-06,
      "loss": 17.8185,
      "step": 580
    },
    {
      "epoch": 1.8864,
      "grad_norm": 36.05081558227539,
      "learning_rate": 7.670940170940172e-06,
      "loss": 18.124,
      "step": 590
    },
    {
      "epoch": 1.9184,
      "grad_norm": 6.897262096405029,
      "learning_rate": 7.457264957264958e-06,
      "loss": 18.2862,
      "step": 600
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 1193.4835205078125,
      "learning_rate": 7.243589743589744e-06,
      "loss": 18.5279,
      "step": 610
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 1404.6722412109375,
      "learning_rate": 7.02991452991453e-06,
      "loss": 18.6072,
      "step": 620
    }
  ],
  "logging_steps": 10,
  "max_steps": 936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4108869872173056.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
